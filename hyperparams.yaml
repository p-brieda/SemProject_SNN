---
#Training
 batch_size: 64
 epochs: 200
 batchesPerVal: 10
 updateVal: 20
 rnnBinSize: 2
 skipLen: 5
 smoothInputs: 1
 cvPart: 'HeldOutBlocks'
 outputDelay: 50
 directionality: 'unidirectional'
 loss: crossentropy
 learning_rate: 0.001
 scheduler: 'StepLR'
 scheduler_gamma: 0.9
 scheduler_step_size: 100
 dropout: 0.0
 weight_decay: 0.00001
 noisy_threshold: 0.0

 var_dropout_s: 0
 var_dropout_t: 0
 zoneout: False
 

# Data augmentation
 constantOffsetSD: 0.0
 randomWalkSD: 0.0
 whiteNoiseSD: 0.6
 train_val_timeSteps: 1200
 test_timeSteps: 7500

# Network
 n_channels: 192
 n_outputs: 32 
 batchnorm: 'none'
 last_nospike: True
 inner_layer: 'fc'
 conv_ker_size: 51
 neuron_count: 512
 layers: 2
 use_bias: True
 network_type: 'default'
 reset_by_subtraction: True
 surrogate_gradient: 'square'
 init_u: 'zero'
 

#Neuron
 Vth: 0.5
 Vth_init_range: 0.1
 Vth_trainable: False

 tau: 0.6 # also mean of distribution if tau_init_range > 0
 tau_init_range: 0.1 # width of uniform distribution / std of normal distribution
 tau_init_distribution: 'uniform' # options are 'uniform' and 'normal'
 tau_trainable: True
 tau_sigmoid: False

 constrain_method: 'forward'
 reset_by_subtraction: True
 surrogate_gradient: 'square'
 init_u: 'zero'


#Dataset
 output_type: 'vel'
 
# Quantization
#  static_quantisation: False
#  quantize_weight: 4 #False or bitcount (4,8,16)
#  quantize_bias: 6
#  quantize_accumulation: False #only used if weights are quantized
#  quantize_activation: False #tau quantisation
#  quantize_membrane_pot: False #membrane potential bitwidth and quantisation
#  quantize_input: 8
#  quantize_input_scaling: 3
 static_quantisation: False
 quantize_weight: False #False or bitcount (4,8,16)
 quantize_bias: False
 quantize_accumulation: False #only used if weights are quantized
 quantize_activation: False #tau quantisation
 quantize_membrane_pot: False #membrane potential bitwidth and quantisation
 quantize_input: False
 quantize_input_scaling: False
 quantisation_scaling: 'max' #'max' or 'mean' for 2.5*mean scaling


 rpr_start_fraction: 0.9
 rpr_step_epochs: 3

#Reporting
 experiment_name: 'baseline_fp_run'
 loss_steps: 100 #how often to report the loss
 output_report: 'C:/Users/pietr/OneDrive/Documenti/PIETRO/ETH/SS24/Semester_project/SNN_project/Report.txt'
 output_plots: 'C:/Users/pietr/OneDrive/Documenti/PIETRO/ETH/SS24/Semester_project/SNN_project/Plots/'
 output_csv: 'C:/Users/pietr/OneDrive/Documenti/PIETRO/ETH/SS24/Semester_project/SNN_project/CSV/'
 results_dir: 'C:/Users/pietr/OneDrive/Documenti/PIETRO/ETH/SS24/Semester_project/SNN_project/trainOutputs/'
 if_plot: False

#General
 seed: 'random' #use 'random' for a non-deterministic seed
 device: cuda:0
 early_stopping: False
 dataset_file: 
#save and load model
 save_model: True
 save_model_dir: 'C:/Users/pietr/OneDrive/Documenti/PIETRO/ETH/SS24/Semester_project/SNN_project/Model/'
 load_model: False 
 load_model_dir: 