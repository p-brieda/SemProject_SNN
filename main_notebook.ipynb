{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from util import getDefaultHyperparams\n",
    "import time\n",
    "from DayDataloaders import create_Dataloaders\n",
    "# Import the metrics file\n",
    "from SequenceLoss import SequenceLoss\n",
    "from network import Net\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 50\n",
      "epochs 200\n",
      "batchesPerVal 10\n",
      "updateVal 20\n",
      "rnnBinSize 2\n",
      "skipLen 5\n",
      "smoothInputs 1\n",
      "cvPart HeldOutBlocks\n",
      "outputDelay 50\n",
      "directionality unidirectional\n",
      "loss crossentropy\n",
      "learning_rate 0.001\n",
      "scheduler LambdaLR\n",
      "scheduler_gamma 0.9\n",
      "scheduler_step_size 10\n",
      "dropout 0\n",
      "weight_decay 0.0\n",
      "noisy_threshold 0.2\n",
      "var_dropout_s 0\n",
      "var_dropout_t 0\n",
      "zoneout False\n",
      "constantOffsetSD 0\n",
      "randomWalkSD 0\n",
      "whiteNoiseSD 0.6\n",
      "train_val_timeSteps 1200\n",
      "test_timeSteps 7500\n",
      "batchnorm none\n",
      "neuron_count 512\n",
      "use_bias True\n",
      "steps 10\n",
      "warmup_steps 2\n",
      "network_type default\n",
      "reset_by_subtraction True\n",
      "surrogate_gradient square\n",
      "init_u zero\n",
      "Vth 0.4\n",
      "Vth_init_range 0.1\n",
      "Vth_trainable False\n",
      "tau 0.6\n",
      "tau_init_range 0.1\n",
      "tau_init_distribution uniform\n",
      "tau_trainable True\n",
      "tau_sigmoid False\n",
      "constrain_method forward\n",
      "output_type vel\n",
      "static_quantisation False\n",
      "quantize_weight False\n",
      "quantize_bias False\n",
      "quantize_accumulation False\n",
      "quantize_activation False\n",
      "quantize_membrane_pot False\n",
      "quantize_input False\n",
      "quantize_input_scaling False\n",
      "quantisation_scaling max\n",
      "rpr_start_fraction 0.9\n",
      "rpr_step_epochs 3\n",
      "experiment_name baseline_fp_run\n",
      "loss_steps 100\n",
      "output_report /home/sem24f8/Semester_project/SNN_Project/SemProject_SNN/Report.txt\n",
      "output_plots /home/sem24f8/Semester_project/SNN_Project/SemProject_SNN/Plots/\n",
      "output_csv /home/sem24f8/Semester_project/SNN_Project/SemProject_SNN/CSV/\n",
      "results_dir /home/sem24f8/Semester_project/SNN_Project/SemProject_SNN/trainOutputs/\n",
      "if_plot False\n",
      "seed 6165\n",
      "device cuda:0\n",
      "early_stopping False\n",
      "dataset_file None\n",
      "save_model True\n",
      "save_model_dir /home/sem24f8/Semester_project/SNN_Project/SemProject_SNN/Model/\n",
      "load_model False\n",
      "load_model_dir None\n",
      "system Linux\n",
      "prepared_data_dir /scratch/sem24f8/dataset/\n",
      "dataDirs ['t5.2019.05.08', 't5.2019.11.25', 't5.2019.12.09', 't5.2019.12.11', 't5.2019.12.18', 't5.2019.12.20', 't5.2020.01.06', 't5.2020.01.08', 't5.2020.01.13', 't5.2020.01.15']\n",
      "sentencesFile_0 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.05.08/sentences.mat\n",
      "singleLettersFile_0 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.05.08/singleLetters.mat\n",
      "labelsFile_0 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2019.05.08_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_0 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2019.05.08_syntheticSentences/\n",
      "cvPartitionFile_0 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_0 t5.2019.05.08\n",
      "sentencesFile_1 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.11.25/sentences.mat\n",
      "singleLettersFile_1 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.11.25/singleLetters.mat\n",
      "labelsFile_1 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2019.11.25_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_1 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2019.11.25_syntheticSentences/\n",
      "cvPartitionFile_1 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_1 t5.2019.11.25\n",
      "sentencesFile_2 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.09/sentences.mat\n",
      "singleLettersFile_2 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.09/singleLetters.mat\n",
      "labelsFile_2 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2019.12.09_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_2 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2019.12.09_syntheticSentences/\n",
      "cvPartitionFile_2 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_2 t5.2019.12.09\n",
      "sentencesFile_3 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.11/sentences.mat\n",
      "singleLettersFile_3 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.11/singleLetters.mat\n",
      "labelsFile_3 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2019.12.11_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_3 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2019.12.11_syntheticSentences/\n",
      "cvPartitionFile_3 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_3 t5.2019.12.11\n",
      "sentencesFile_4 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.18/sentences.mat\n",
      "singleLettersFile_4 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.18/singleLetters.mat\n",
      "labelsFile_4 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2019.12.18_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_4 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2019.12.18_syntheticSentences/\n",
      "cvPartitionFile_4 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_4 t5.2019.12.18\n",
      "sentencesFile_5 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.20/sentences.mat\n",
      "singleLettersFile_5 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2019.12.20/singleLetters.mat\n",
      "labelsFile_5 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2019.12.20_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_5 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2019.12.20_syntheticSentences/\n",
      "cvPartitionFile_5 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_5 t5.2019.12.20\n",
      "sentencesFile_6 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.06/sentences.mat\n",
      "singleLettersFile_6 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.06/singleLetters.mat\n",
      "labelsFile_6 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2020.01.06_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_6 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2020.01.06_syntheticSentences/\n",
      "cvPartitionFile_6 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_6 t5.2020.01.06\n",
      "sentencesFile_7 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.08/sentences.mat\n",
      "singleLettersFile_7 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.08/singleLetters.mat\n",
      "labelsFile_7 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2020.01.08_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_7 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2020.01.08_syntheticSentences/\n",
      "cvPartitionFile_7 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_7 t5.2020.01.08\n",
      "sentencesFile_8 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.13/sentences.mat\n",
      "singleLettersFile_8 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.13/singleLetters.mat\n",
      "labelsFile_8 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2020.01.13_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_8 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2020.01.13_syntheticSentences/\n",
      "cvPartitionFile_8 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_8 t5.2020.01.13\n",
      "sentencesFile_9 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.15/sentences.mat\n",
      "singleLettersFile_9 /home/sem24f8/Semester_project/handwritingBCIData/Datasets/t5.2020.01.15/singleLetters.mat\n",
      "labelsFile_9 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step2_HMMLabels/HeldOutBlocks/t5.2020.01.15_timeSeriesLabels.mat\n",
      "syntheticDatasetDir_9 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/Step3_SyntheticSentences/HeldOutBlocks/t5.2020.01.15_syntheticSentences/\n",
      "cvPartitionFile_9 /home/sem24f8/Semester_project/handwritingBCIData/RNNTrainingSteps/trainTestPartitions_HeldOutBlocks.mat\n",
      "sessionName_9 t5.2020.01.15\n",
      "n_channels 192\n",
      "n_outputs 32\n",
      "id 525268\n"
     ]
    }
   ],
   "source": [
    "params = pickle.load(open('trainOutputs/hyperparam_525268.pickle', 'rb'))\n",
    "for key in params:\n",
    "    print(key, params[key]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([10, 1, 512])\n",
      "Output tensor shape: torch.Size([10, 512])\n",
      "Output tensor: tensor([[ 3.5886e-01,  5.5119e-01,  4.5247e-01,  ...,  9.8427e+01,\n",
      "         -1.4705e+01, -4.4690e+01],\n",
      "        [ 9.8831e+01, -4.9991e+01,  9.9393e+01,  ...,  1.9737e+02,\n",
      "         -2.9090e+01, -8.9031e+01],\n",
      "        [ 1.9730e+02, -1.0053e+02,  1.9833e+02,  ...,  2.9631e+02,\n",
      "         -4.3475e+01, -1.3337e+02],\n",
      "        ...,\n",
      "        [ 6.8966e+02, -3.5324e+02,  6.9304e+02,  ...,  7.9101e+02,\n",
      "         -1.1540e+02, -3.5508e+02],\n",
      "        [ 7.8814e+02, -4.0379e+02,  7.9198e+02,  ...,  8.8995e+02,\n",
      "         -1.2978e+02, -3.9942e+02],\n",
      "        [ 8.8661e+02, -4.5433e+02,  8.9092e+02,  ...,  9.8889e+02,\n",
      "         -1.4417e+02, -4.4376e+02]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create the Conv1d layer\n",
    "conv = nn.Conv1d(1, 1, kernel_size=5, padding=5//2)\n",
    "\n",
    "# Create a sample input tensor\n",
    "input_array = np.arange(5120).reshape(10, 512)\n",
    "input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
    "input_tensor = input_tensor.unsqueeze(1)  # Add a dummy sequence_length dimension\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "\n",
    "# Apply the convolution operation\n",
    "output_tensor = conv(input_tensor)\n",
    "output_tensor = output_tensor.squeeze(1)  # Remove the dummy sequence_length dimension\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(\"Output tensor shape:\", output_tensor.shape)\n",
    "\n",
    "print(\"Output tensor:\", output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prepared data from dir\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load the prepared data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading prepared data from dir\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m prepared_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_data_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprepared_data.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData loaded\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m manual_prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pietr\\anaconda3\\envs\\snnproject\\lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1027\u001b[0m                      map_location,\n\u001b[0;32m   1028\u001b[0m                      pickle_module,\n\u001b[0;32m   1029\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1030\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pietr\\anaconda3\\envs\\snnproject\\lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pietr\\anaconda3\\envs\\snnproject\\lib\\site-packages\\torch\\serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\pietr\\anaconda3\\envs\\snnproject\\lib\\site-packages\\torch\\serialization.py:1373\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1371\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1373\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams = getDefaultHyperparams()\n",
    "\n",
    "if hyperparams['system'] == 'Linux':\n",
    "    prepared_data_dir = '/scratch/sem24f8/dataset/'\n",
    "else:\n",
    "    prepared_data_dir = 'dataset/'\n",
    "\n",
    "hyperparams['prepared_data_dir'] = prepared_data_dir\n",
    "\n",
    "hyperparams['n_channels'] = 192\n",
    "hyperparams['n_outputs'] = 32\n",
    "hyperparams['batchSize'] = 2\n",
    "\n",
    "# Load the prepared data\n",
    "print('Loading prepared data from dir')\n",
    "prepared_data = torch.load(prepared_data_dir + 'prepared_data.pth')\n",
    "print('Data loaded')\n",
    "\n",
    "manual_prep = False\n",
    "# loading the testing dataset and creating a DataLoader for each day\n",
    "Test_finite_loader = create_Dataloaders(manual_prep, hyperparams, days=np.arange(10), mode='testing')\n",
    "test_loaders = Test_finite_loader.getDataloaders()\n",
    "viable_test_days = Test_finite_loader.getViableDays()\n",
    "print('Testing dataloaders ready')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n",
      "Device: cuda:0\n",
      "Testing on day t5.2019.11.25\n",
      "Testing progress: |# 8.0996\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# test the model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m criterion \u001b[38;5;241m=\u001b[39m SequenceLoss(hyperparams)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtestModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviable_test_days\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pietr\\OneDrive\\Documenti\\PIETRO\\ETH\\SS24\\Semester_project\\SNN_project\\util.py:429\u001b[0m, in \u001b[0;36mtestModel\u001b[1;34m(model, test_loaders, viable_test_days, criterion, hyperparams, device)\u001b[0m\n\u001b[0;32m    425\u001b[0m testIdx \u001b[38;5;241m=\u001b[39m cvPartFile[hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataDirs\u001b[39m\u001b[38;5;124m'\u001b[39m][dayIdx]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    427\u001b[0m sentenceDat \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mloadmat(hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentencesFile_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(dayIdx)])\n\u001b[1;32m--> 429\u001b[0m errCounts, decSentences \u001b[38;5;241m=\u001b[39m \u001b[43mevaluateSNNOutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentenceDat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumTimeBinsPerSentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtestIdx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrnnBinSize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputDelay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43msentenceDat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentencePrompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtestIdx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcharDef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcharStartThresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcharStartDelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m#save decoded sentences, character error rates and word error rates for later summarization\u001b[39;00m\n\u001b[0;32m    436\u001b[0m saveDict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\pietr\\OneDrive\\Documenti\\PIETRO\\ETH\\SS24\\Semester_project\\SNN_project\\Evaluation.py:14\u001b[0m, in \u001b[0;36mevaluateSNNOutput\u001b[1;34m(snnOutput, numBinsPerSentence, trueText, charDef, charStartThresh, charStartDelay)\u001b[0m\n\u001b[0;32m     11\u001b[0m charStart \u001b[38;5;241m=\u001b[39m snnOutput[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#convert output to character strings\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m decStr \u001b[38;5;241m=\u001b[39m \u001b[43mdecodeCharStr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlgit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharStart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharStartThresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharStartDelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnumBinsPerSentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharDef\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcharListAbbr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m allErrCounts \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     18\u001b[0m allErrCounts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharCounts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;28mlen\u001b[39m(trueText)])\n",
      "File \u001b[1;32mc:\\Users\\pietr\\OneDrive\\Documenti\\PIETRO\\ETH\\SS24\\Semester_project\\SNN_project\\Evaluation.py:63\u001b[0m, in \u001b[0;36mdecodeCharStr\u001b[1;34m(logitMatrix, transSignal, transThresh, transDelay, numBinsPerTrial, charList)\u001b[0m\n\u001b[0;32m     60\u001b[0m letTrans \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mexpit(transSignal[v,:])\n\u001b[0;32m     62\u001b[0m endIdx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mceil(numBinsPerTrial[v])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m letTrans \u001b[38;5;241m=\u001b[39m \u001b[43mletTrans\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mendIdx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     65\u001b[0m transIdx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(np\u001b[38;5;241m.\u001b[39mlogical_and(letTrans[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m<\u001b[39mtransThresh, letTrans[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m>\u001b[39mtransThresh))\n\u001b[0;32m     66\u001b[0m transIdx \u001b[38;5;241m=\u001b[39m transIdx[:,\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "id = '107776'\n",
    "\n",
    "from util import testModel\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "    device = torch.device(hyperparams['device'])\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = Net(hyperparams).to(device)\n",
    "state_dict = torch.load(f'Model/model_inf_{id}.pth', map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# test the model\n",
    "criterion = SequenceLoss(hyperparams)\n",
    "testModel(model, test_loaders, viable_test_days, criterion, hyperparams, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snnproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
