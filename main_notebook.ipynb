{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from util import getDefaultHyperparams, extractBatch, trainModel, validateModel, testModel\n",
    "from SingleDataloader import DataProcessing, CustomBatchSampler, TestBatchSampler\n",
    "from DayDataloaders import create_Dataloaders, DayInfiniteIterators\n",
    "from PrepareData import PrepareData\n",
    "from torch.utils.data import DataLoader\n",
    "from network import Net, RSNNet\n",
    "from SequenceLoss import SequenceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please set: /n hyperparams['outputDir']\n"
     ]
    }
   ],
   "source": [
    "hyperparams = getDefaultHyperparams()\n",
    "hyperparams['batch_size'] = 20\n",
    "hyperparams['train_val_timeSteps'] = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded\n",
      "Validation data loaded\n",
      "Testing data loaded\n"
     ]
    }
   ],
   "source": [
    "prepared_data = PrepareData(hyperparams)\n",
    "\n",
    "# loading the training dataset and creating a DataLoader\n",
    "Train_dataset = DataProcessing(hyperparams, prepared_data, mode='training')\n",
    "trainDayBatch_Sampler = CustomBatchSampler(Train_dataset.getDaysIdx(), hyperparams['batch_size'])\n",
    "train_loader = DataLoader(Train_dataset, batch_sampler = trainDayBatch_Sampler , num_workers=2)\n",
    "print('Training data loaded')\n",
    "# \n",
    "\n",
    "\n",
    "# loading the validation dataset and creating a DataLoader\n",
    "Val_dataset = DataProcessing(hyperparams, prepared_data, mode='validation')\n",
    "valDayBatch_Sampler = CustomBatchSampler(Val_dataset.getDaysIdx(), hyperparams['batch_size'])\n",
    "val_loader = DataLoader(Val_dataset, batch_sampler = valDayBatch_Sampler, num_workers=2)\n",
    "print('Validation data loaded')\n",
    "\n",
    "# loading the testing dataset and creating a DataLoader for each day\n",
    "Test_finite_loader = create_Dataloaders(hyperparams, days=np.arange(10), mode='testing')\n",
    "test_loaders = Test_finite_loader.getDataloaders()\n",
    "viable_test_days = Test_finite_loader.getViableDays()\n",
    "print('Testing data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "# Device selection\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "# Model creation\n",
    "model = Net(hyperparams)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = SequenceLoss(hyperparams)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparams['learning_rate'], \n",
    "                              betas= (0.9, 0.999), eps=1e-08, \n",
    "                              weight_decay=hyperparams['weight_decay'], amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "trainModel() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Training epoch\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Validation epoch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m validateModel(model, val_loader, criterion, hyperparams, device)\n",
      "\u001b[1;31mTypeError\u001b[0m: trainModel() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "training_start = time.time()\n",
    "epochs = hyperparams['epochs']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Start epoch timer\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "\n",
    "    # Training epoch\n",
    "    train_loss = trainModel(model, train_loader , optimizer, criterion, hyperparams, device)\n",
    "    # Validation epoch\n",
    "    val_loss, val_acc = validateModel(model, val_loader, criterion, hyperparams, device)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(f\"Epoch time: {epoch_end - epoch_start:.2f} s\")\n",
    "    # Print results of the epoch\n",
    "    print(f\"Train loss: {train_loss:.4f} | Validation loss: {val_loss:.4f} | Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "training_end = time.time()\n",
    "print('Training time: ', training_end - training_start)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start testing\n",
    "test_start = time.time()\n",
    "testModel(model, test_loaders, viable_test_days,hyperparams, device)\n",
    "test_end = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snnproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
