14:14:17,58 ---  
14:14:17,58 --- =========================================================================
14:14:17,58 --- New run started with id 394634
14:14:17,58 ---  
14:14:29,650 --- Loading prepared data from dir
14:14:29,650 --- Training dataloaders ready
14:14:29,650 --- Validation dataloaders ready
14:14:42,335 --- Testing dataloaders ready
14:14:42,335 ---  
14:14:43,283 --- Using cuda:0
14:14:45,17 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
14:14:45,17 --- Number of training batches: 30
14:14:45,17 --- Number of validation batches: 14
14:14:45,17 --- Number of epochs: 10
14:14:45,17 --- Scheduler: StepLR(step_size=10, gamma=0.1)
14:14:45,17 ---  
14:14:45,17 --- Epoch: 1
14:21:09,756 --- Epoch time: 384.74 s
14:21:09,760 --- Train loss: 2.9132 | Validation loss: 2.6502 | Validation accuracy: 0.1628
14:21:09,760 ---  
14:21:09,760 --- Epoch: 2
14:27:42,615 --- Epoch time: 392.85 s
14:27:42,615 --- Train loss: 2.6877 | Validation loss: 2.7718 | Validation accuracy: 0.1641
14:27:42,615 ---  
14:27:42,615 --- Epoch: 3
14:34:12,290 --- Epoch time: 389.67 s
14:34:12,290 --- Train loss: 2.6841 | Validation loss: 2.7798 | Validation accuracy: 0.1640
14:34:12,290 ---  
14:34:12,290 --- Epoch: 4
14:40:36,337 --- Epoch time: 384.05 s
14:40:36,337 --- Train loss: 2.6764 | Validation loss: 2.8196 | Validation accuracy: 0.1696
14:40:36,337 ---  
14:40:36,337 --- Epoch: 5
15:13:13,759 ---  
15:13:13,764 --- =========================================================================
15:13:13,764 --- New run started with id 106885
15:13:13,764 ---  
15:13:37,992 --- Loading prepared data from dir
15:13:38,75 --- Training dataloaders ready
15:13:38,75 --- Validation dataloaders ready
15:14:09,688 --- Testing dataloaders ready
15:14:09,688 ---  
15:14:09,843 --- Using cuda:0
15:14:13,316 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
15:14:13,316 --- Number of training batches: 30
15:14:13,316 --- Number of validation batches: 14
15:14:13,316 --- Number of epochs: 10
15:14:13,316 --- Scheduler: StepLR(step_size=10, gamma=0.1)
15:14:13,316 ---  
15:14:13,316 --- Epoch: 1
15:20:22,851 --- Epoch time: 369.53 s
15:20:22,860 --- Train loss: 2.8744 | Validation loss: 2.7271 | Validation accuracy: 0.1649
15:20:22,860 ---  
15:20:22,860 --- Epoch: 2
15:26:02,216 --- Epoch time: 339.35 s
15:26:02,218 --- Train loss: 2.6846 | Validation loss: 2.7205 | Validation accuracy: 0.1640
15:26:02,218 ---  
15:26:02,219 --- Epoch: 3
15:31:36,940 --- Epoch time: 334.72 s
15:31:36,944 --- Train loss: 2.8073 | Validation loss: 2.8192 | Validation accuracy: 0.1653
15:31:36,944 ---  
15:31:36,945 --- Epoch: 4
15:37:05,92 --- Epoch time: 328.15 s
15:37:05,93 --- Train loss: 2.7013 | Validation loss: 2.7331 | Validation accuracy: 0.1633
15:37:05,93 ---  
15:37:05,93 --- Epoch: 5
15:42:32,249 --- Epoch time: 327.15 s
15:42:32,250 --- Train loss: 2.6762 | Validation loss: 2.7166 | Validation accuracy: 0.1553
15:42:32,251 ---  
15:42:32,251 --- Epoch: 6
15:48:06,545 --- Epoch time: 334.29 s
15:48:06,558 --- Train loss: 2.7253 | Validation loss: 2.7384 | Validation accuracy: 0.1655
15:48:06,559 ---  
15:48:06,559 --- Epoch: 7
15:53:44,381 --- Epoch time: 337.82 s
15:53:44,383 --- Train loss: 2.6478 | Validation loss: 2.7993 | Validation accuracy: 0.1644
15:53:44,383 ---  
15:53:44,384 --- Epoch: 8
15:59:22,479 --- Epoch time: 338.10 s
15:59:22,482 --- Train loss: 2.6635 | Validation loss: 2.8200 | Validation accuracy: 0.1207
15:59:22,482 ---  
15:59:22,482 --- Epoch: 9
16:04:58,572 --- Epoch time: 336.09 s
16:04:58,574 --- Train loss: 2.7490 | Validation loss: 2.6876 | Validation accuracy: 0.1653
16:04:58,575 ---  
16:04:58,576 --- Epoch: 10
16:10:30,245 --- Epoch time: 331.67 s
16:10:30,249 --- Train loss: 2.5507 | Validation loss: 2.6058 | Validation accuracy: 0.1691
16:10:30,249 ---  
16:10:30,249 --- Training time: 56.28 mins
16:10:30,297 --- Model saved
09:48:57,226 ---  
16:23:10,751 ---  
16:23:10,751 --- =========================================================================
16:23:10,751 --- New run started with id 451123
16:23:10,751 ---  
16:27:01,613 ---  
16:27:01,613 --- =========================================================================
16:27:01,613 --- New run started with id 915640
16:27:01,613 ---  
16:27:12,968 --- Loading prepared data from dir
16:27:12,968 --- Training dataloaders ready
16:27:12,968 --- Validation dataloaders ready
16:27:22,928 --- Testing dataloaders ready
16:27:22,928 ---  
16:27:23,0 --- Using cuda:0
16:27:24,356 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:27:24,356 --- Number of training batches: 30
16:27:24,356 --- Number of validation batches: 14
16:27:24,356 --- Number of epochs: 10
16:27:24,356 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:27:24,356 ---  
16:27:24,356 --- Epoch: 1
16:32:17,482 ---  
16:32:17,482 --- =========================================================================
16:32:17,482 --- New run started with id 434948
16:32:17,482 ---  
16:32:28,469 --- Loading prepared data from dir
16:32:28,470 --- Training dataloaders ready
16:32:28,471 --- Validation dataloaders ready
16:32:37,397 --- Testing dataloaders ready
16:32:37,397 ---  
16:32:37,445 --- Using cuda:0
16:32:38,362 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:32:38,362 --- Number of training batches: 30
16:32:38,362 --- Number of validation batches: 14
16:32:38,362 --- Number of epochs: 10
16:32:38,362 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:32:38,362 ---  
16:32:38,362 --- Epoch: 1
16:41:59,731 ---  
16:41:59,731 --- =========================================================================
16:41:59,731 --- New run started with id 452988
16:41:59,731 ---  
16:42:09,147 --- Loading prepared data from dir
16:42:09,147 --- Training dataloaders ready
16:42:09,147 --- Validation dataloaders ready
16:42:18,596 --- Testing dataloaders ready
16:42:18,596 ---  
16:42:18,641 --- Using cuda:0
16:42:19,577 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:42:19,577 --- Number of training batches: 30
16:42:19,577 --- Number of validation batches: 14
16:42:19,577 --- Number of epochs: 10
16:42:19,577 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:42:19,577 ---  
16:42:19,577 --- Epoch: 1
16:44:04,278 ---  
16:44:04,278 --- =========================================================================
16:44:04,278 --- New run started with id 538834
16:44:04,278 ---  
16:44:13,707 --- Loading prepared data from dir
16:44:13,722 --- Training dataloaders ready
16:44:13,722 --- Validation dataloaders ready
16:44:22,723 --- Testing dataloaders ready
16:44:22,723 ---  
16:44:22,769 --- Using cuda:0
16:44:23,680 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:44:23,680 --- Number of training batches: 30
16:44:23,680 --- Number of validation batches: 14
16:44:23,680 --- Number of epochs: 10
16:44:23,680 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:44:23,680 ---  
16:44:23,680 --- Epoch: 1
16:45:03,155 ---  
16:45:03,155 --- =========================================================================
16:45:03,155 --- New run started with id 664905
16:45:03,155 ---  
16:45:12,355 --- Loading prepared data from dir
16:45:12,355 --- Training dataloaders ready
16:45:12,355 --- Validation dataloaders ready
16:45:21,282 --- Testing dataloaders ready
16:45:21,282 ---  
16:45:21,315 --- Using cuda:0
16:45:22,255 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:45:22,255 --- Number of training batches: 30
16:45:22,255 --- Number of validation batches: 14
16:45:22,255 --- Number of epochs: 10
16:45:22,255 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:45:22,255 ---  
16:45:22,255 --- Epoch: 1
16:47:51,692 --- Epoch time: 149.44 s
16:47:51,702 --- Train loss: 2.8342 | Validation loss: 2.8011 | Validation accuracy: 0.1502
16:47:51,702 ---  
16:47:51,702 --- Epoch: 2
16:50:26,657 --- Epoch time: 154.95 s
16:50:26,657 --- Train loss: 2.7936 | Validation loss: 2.6426 | Validation accuracy: 0.1682
16:50:26,657 ---  
16:50:26,657 --- Epoch: 3
16:52:55,468 --- Epoch time: 148.81 s
16:52:55,469 --- Train loss: 2.6431 | Validation loss: 2.8104 | Validation accuracy: 0.1678
16:52:55,469 ---  
16:52:55,469 --- Epoch: 4
16:55:20,883 --- Epoch time: 145.41 s
16:55:20,883 --- Train loss: 2.6178 | Validation loss: 2.7072 | Validation accuracy: 0.1640
16:55:20,884 ---  
16:55:20,884 --- Epoch: 5
16:57:47,952 --- Epoch time: 147.07 s
16:57:47,954 --- Train loss: 2.6719 | Validation loss: 2.7374 | Validation accuracy: 0.1568
16:57:47,955 ---  
16:57:47,955 --- Epoch: 6
17:00:15,5 --- Epoch time: 147.05 s
17:00:15,6 --- Train loss: 2.6401 | Validation loss: 2.8167 | Validation accuracy: 0.1636
17:00:15,6 ---  
17:00:15,7 --- Epoch: 7
17:02:41,359 --- Epoch time: 146.35 s
17:02:41,361 --- Train loss: 2.7423 | Validation loss: 2.6569 | Validation accuracy: 0.1498
17:02:41,361 ---  
17:02:41,361 --- Epoch: 8
10:53:05,739 ---  
10:53:05,739 --- =========================================================================
10:53:05,739 --- New run started with id 525137
10:53:05,739 ---  
10:53:07,653 --- Loading prepared data from dir
10:53:17,771 --- Training dataloaders ready
10:53:17,771 --- Validation dataloaders ready
10:53:17,771 --- Loading prepared testing data from dir
10:53:27,263 --- Testing dataloaders ready
10:53:27,263 ---  
10:53:27,324 --- Using cuda:0
10:53:28,623 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
10:53:28,623 --- Number of training batches: 11
10:53:28,623 --- Number of validation batches: 7
10:53:28,623 --- Number of epochs: 100
10:53:28,623 --- Scheduler: StepLR(step_size=10, gamma=0.1)
10:53:28,623 ---  
10:53:28,623 --- Epoch: 1
10:56:54,284 --- Epoch time: 205.66 s
10:56:54,284 --- Training loss: 2.9486 | Training accuracy: 0.10586257414384322 | Validation loss: 3.3072 | Validation accuracy: 0.0611
10:56:54,284 ---  
10:56:54,284 --- Epoch: 2
11:00:19,934 --- Epoch time: 205.65 s
11:00:19,934 --- Training loss: 2.7202 | Training accuracy: 0.16262356801466507 | Validation loss: 2.9285 | Validation accuracy: 0.0797
11:00:19,934 ---  
11:00:19,941 --- Epoch: 3
11:03:40,558 --- Epoch time: 200.62 s
11:03:40,559 --- Training loss: 2.7241 | Training accuracy: 0.1406726078553633 | Validation loss: 2.8814 | Validation accuracy: 0.1200
11:03:40,559 ---  
11:03:40,560 --- Epoch: 4
12:14:43,348 ---  
12:14:43,348 --- =========================================================================
12:14:43,349 --- New run started with id 908027
12:14:43,350 ---  
12:14:45,267 --- Loading prepared data from dir
12:14:54,34 --- Loading prepared testing data from dir
12:15:04,753 --- Training dataloaders ready
12:15:04,753 ---  
12:15:04,753 --- Loading prepared testing data from dir
12:15:20,579 --- Validation dataloaders ready
12:15:20,579 ---  
12:15:20,579 --- Loading prepared testing data from dir
12:15:36,55 --- Testing dataloaders ready
12:15:36,55 ---  
12:15:36,181 --- Using cuda:0
12:15:38,70 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:15:38,70 --- Total training batches: 600
12:15:38,70 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:15:38,70 ---  
12:16:09,250 ---  
12:16:09,250 --- =========================================================================
12:16:09,250 --- New run started with id 222127
12:16:09,250 ---  
12:16:10,769 --- Loading prepared data from dir
12:16:18,159 --- Loading prepared testing data from dir
12:16:27,113 --- Training dataloaders ready
12:16:27,113 ---  
12:16:27,113 --- Loading prepared testing data from dir
12:16:41,32 --- Validation dataloaders ready
12:16:41,33 ---  
12:16:41,33 --- Loading prepared testing data from dir
12:16:56,104 --- Testing dataloaders ready
12:16:56,104 ---  
12:16:56,189 --- Using cuda:0
12:16:57,649 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:16:57,649 --- Total training batches: 600
12:16:57,649 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:16:57,649 ---  
12:20:08,411 ---  
12:20:08,411 --- =========================================================================
12:20:08,411 --- New run started with id 438532
12:20:08,411 ---  
12:20:12,262 --- Loading prepared data from dir
12:20:19,578 --- Loading prepared testing data from dir
12:20:28,902 --- Training dataloaders ready
12:20:28,902 ---  
12:20:28,902 --- Loading prepared testing data from dir
12:20:40,647 --- Validation dataloaders ready
12:20:40,647 ---  
12:20:40,662 --- Loading prepared testing data from dir
12:20:53,696 --- Testing dataloaders ready
12:20:53,699 ---  
12:20:53,831 --- Using cuda:0
12:20:55,784 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:20:55,784 --- Total training batches: 600
12:20:55,784 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:20:55,784 ---  
12:30:50,263 ---  
12:30:50,264 --- =========================================================================
12:30:50,264 --- New run started with id 590950
12:30:50,264 ---  
12:33:00,571 --- Training dataloaders ready
12:33:00,571 ---  
12:35:18,805 --- Validation dataloaders ready
12:35:18,805 ---  
12:37:41,819 --- Testing dataloaders ready
12:37:41,819 ---  
12:37:41,920 --- Using cuda:0
12:37:43,282 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:37:43,282 --- Total training batches: 600
12:37:43,282 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:37:43,282 ---  
12:39:08,478 ---  
12:39:08,478 --- =========================================================================
12:39:08,478 --- New run started with id 439961
12:39:08,478 ---  
12:39:11,756 --- Loading prepared training data from dir
12:39:19,284 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:39:19,306 --- Training dataloaders ready
12:39:19,306 ---  
12:39:19,306 --- Loading prepared validation data from dir
12:39:29,83 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:39:29,83 --- Validation dataloaders ready
12:39:29,83 ---  
12:39:29,83 --- Loading prepared testing data from dir
12:39:40,969 --- Testing dataloaders ready
12:39:40,969 ---  
12:39:41,34 --- Using cuda:0
12:39:43,75 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:39:43,75 --- Total training batches: 600
12:39:43,75 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:39:43,75 ---  
12:40:01,119 --- Batch 1/600 | Training loss: 3.421 | Training accuracy: 0.032
12:46:39,622 ---  
12:46:39,622 --- =========================================================================
12:46:39,622 --- New run started with id 335772
12:46:39,622 ---  
12:46:41,885 --- Loading prepared training data from dir
12:46:50,755 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:46:50,755 --- Training dataloaders ready
12:46:50,755 ---  
12:46:50,755 --- Loading prepared validation data from dir
12:47:00,593 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:47:00,593 --- Validation dataloaders ready
12:47:00,593 ---  
12:47:00,593 --- Loading prepared testing data from dir
12:47:12,15 --- Testing dataloaders ready
12:47:12,16 ---  
12:47:12,120 --- Using cuda:0
12:47:14,4 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:47:14,4 --- Total training batches: 600
12:47:14,4 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:47:14,5 ---  
12:47:33,9 --- Batch 1/600 | Training loss: 3.452 | Training accuracy: 0.097
12:47:33,749 --- Validation loss: 3.418 | Validation accuracy: 0.018
12:49:37,617 ---  
12:49:37,617 --- =========================================================================
12:49:37,617 --- New run started with id 218091
12:49:37,617 --- ID: 218091
12:49:37,617 --- Infinite iterators strategy
12:49:39,647 --- Loading prepared training data from dir
12:49:46,887 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:49:46,887 --- Training dataloaders ready
12:49:46,887 ---  
12:49:46,892 --- Loading prepared validation data from dir
12:49:55,763 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:49:55,763 --- Validation dataloaders ready
12:49:55,763 ---  
12:49:55,763 --- Loading prepared testing data from dir
12:50:06,559 --- Testing dataloaders ready
12:50:06,559 ---  
12:50:06,670 --- Using cuda:0
12:50:08,232 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:50:08,232 --- Total training batches: 600
12:50:08,232 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:50:08,232 ---  
12:50:27,101 --- Batch 1/600 | Training loss: 3.436 | Training accuracy: 0.053
12:50:27,813 --- Validation loss: 3.894 | Validation accuracy: 0.000
12:53:47,28 ---  
12:53:47,28 --- =========================================================================
12:53:47,28 --- New run started with id 810262
12:53:47,28 --- ID: 810262
12:53:47,28 --- Infinite iterators strategy
12:53:48,857 --- Loading prepared training data from dir
12:53:55,842 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:53:55,842 --- Training dataloaders ready
12:53:55,842 ---  
12:53:55,842 --- Loading prepared validation data from dir
12:54:04,685 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:54:04,685 --- Validation dataloaders ready
12:54:04,685 ---  
12:54:04,685 --- Loading prepared testing data from dir
12:54:15,504 --- Testing dataloaders ready
12:54:15,504 ---  
12:54:15,565 --- Using cuda:0
12:54:16,826 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:54:16,826 --- Total training batches: 600
12:54:16,826 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:54:16,826 ---  
12:54:35,646 --- Batch 1/600 | Training loss: 3.848 | Training accuracy: 0.035
12:54:36,441 --- Validation loss: 3.481 | Validation accuracy: 0.049
12:57:28,320 --- Batch 11/600 | Training loss: 2.837 | Training accuracy: 0.182
13:00:13,700 --- Batch 21/600 | Training loss: 2.400 | Training accuracy: 0.165
13:02:47,814 --- Batch 31/600 | Training loss: 2.912 | Training accuracy: 0.180
13:11:33,810 ---  
13:11:33,810 --- =========================================================================
13:11:33,810 --- New run started with id 306177
13:11:33,810 --- ID: 306177
13:11:33,813 --- Infinite iterators strategy
13:15:47,203 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
13:15:47,204 --- Training dataloaders ready
13:15:47,212 ---  
13:20:01,372 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
13:20:01,374 --- Validation dataloaders ready
13:20:01,376 ---  
13:24:39,935 --- Testing dataloaders ready
13:24:39,936 ---  
13:24:40,864 --- Using cuda:0
13:24:54,29 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
13:24:54,30 --- Total training batches: 600
13:24:54,30 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
13:24:54,32 ---  
13:25:05,706 --- Batch 1/600 | Training loss: 3.6264 | Training accuracy: 0.0249
13:25:06,639 --- Validation loss: 3.1047 | Validation accuracy: 0.0160
13:26:29,302 --- Batch 11/600 | Training loss: 2.8791 | Training accuracy: 0.1582
13:27:51,428 --- Batch 21/600 | Training loss: 2.6900 | Training accuracy: 0.1504
13:29:04,456 --- Batch 31/600 | Training loss: 2.7404 | Training accuracy: 0.1675
13:29:05,479 --- Validation loss: 2.7770 | Validation accuracy: 0.1267
13:30:15,528 --- Batch 41/600 | Training loss: 2.8517 | Training accuracy: 0.1727
13:31:37,40 --- Batch 51/600 | Training loss: 2.7256 | Training accuracy: 0.1573
13:32:58,69 --- Batch 61/600 | Training loss: 2.3862 | Training accuracy: 0.1814
13:32:59,38 --- Validation loss: 2.6972 | Validation accuracy: 0.1702
13:34:17,652 --- Batch 71/600 | Training loss: 2.9401 | Training accuracy: 0.2031
13:35:42,478 --- Batch 81/600 | Training loss: 2.4854 | Training accuracy: 0.2143
13:37:09,63 --- Batch 91/600 | Training loss: 2.4946 | Training accuracy: 0.2444
13:37:10,104 --- Validation loss: 2.9066 | Validation accuracy: 0.1465
13:38:25,987 --- Batch 101/600 | Training loss: 2.3956 | Training accuracy: 0.2028
13:39:47,298 --- Batch 111/600 | Training loss: 2.5174 | Training accuracy: 0.1978
13:41:08,610 --- Batch 121/600 | Training loss: 2.4668 | Training accuracy: 0.2306
13:41:09,540 --- Validation loss: 2.5154 | Validation accuracy: 0.1539
13:42:30,931 --- Batch 131/600 | Training loss: 2.4338 | Training accuracy: 0.2413
13:43:49,766 --- Batch 141/600 | Training loss: 2.3417 | Training accuracy: 0.2259
13:45:10,526 --- Batch 151/600 | Training loss: 2.2850 | Training accuracy: 0.2524
13:45:11,447 --- Validation loss: 2.3363 | Validation accuracy: 0.1381
13:46:26,424 --- Batch 161/600 | Training loss: 2.3518 | Training accuracy: 0.2459
13:47:51,334 --- Batch 171/600 | Training loss: 2.5328 | Training accuracy: 0.2674
13:49:16,376 --- Batch 181/600 | Training loss: 2.3510 | Training accuracy: 0.2891
13:49:17,277 --- Validation loss: 2.9704 | Validation accuracy: 0.1029
13:50:32,762 --- Batch 191/600 | Training loss: 2.0786 | Training accuracy: 0.2781
13:51:50,538 --- Batch 201/600 | Training loss: 2.3789 | Training accuracy: 0.2930
13:53:12,421 --- Batch 211/600 | Training loss: 2.4135 | Training accuracy: 0.3033
13:53:13,265 --- Validation loss: 3.0979 | Validation accuracy: 0.1705
13:54:28,301 --- Batch 221/600 | Training loss: 2.3167 | Training accuracy: 0.3037
13:55:50,91 --- Batch 231/600 | Training loss: 2.0943 | Training accuracy: 0.3337
13:57:02,462 --- Batch 241/600 | Training loss: 2.1898 | Training accuracy: 0.3239
13:57:03,388 --- Validation loss: 2.7058 | Validation accuracy: 0.1175
14:09:20,175 ---  
14:09:20,175 --- =========================================================================
14:09:20,175 --- New run started with id 938171
14:09:20,175 --- ID: 938171
14:09:20,177 --- Infinite iterators strategy
14:09:22,732 --- Loading prepared training data from dir
14:10:33,434 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
14:10:33,437 --- Training dataloaders ready
14:10:33,438 ---  
14:10:33,440 --- Loading prepared validation data from dir
14:11:53,87 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
14:11:53,88 --- Validation dataloaders ready
14:11:53,88 ---  
14:11:53,88 --- Loading prepared testing data from dir
14:13:03,114 --- Testing dataloaders ready
14:13:03,160 ---  
14:13:03,508 --- Using cuda:0
14:13:15,399 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
14:13:15,399 --- Total training batches: 600
14:13:15,399 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
14:13:15,401 ---  
14:13:27,499 --- Batch 1/600 | Training loss: 3.6220 | Training accuracy: 0.0274
14:13:28,154 --- Validation loss: 3.3187 | Validation accuracy: 0.1654
14:14:46,937 --- Batch 11/600 | Training loss: 2.8123 | Training accuracy: 0.1503
14:16:08,555 --- Batch 21/600 | Training loss: 2.6821 | Training accuracy: 0.1731
14:17:20,323 --- Batch 31/600 | Training loss: 2.7886 | Training accuracy: 0.1853
14:17:21,54 --- Validation loss: 3.4346 | Validation accuracy: 0.1326
14:18:43,805 --- Batch 41/600 | Training loss: 2.6028 | Training accuracy: 0.2101
14:19:59,174 --- Batch 51/600 | Training loss: 2.5060 | Training accuracy: 0.1829
14:21:19,500 --- Batch 61/600 | Training loss: 2.6635 | Training accuracy: 0.1918
14:21:20,238 --- Validation loss: 2.2461 | Validation accuracy: 0.1461
14:22:41,902 --- Batch 71/600 | Training loss: 2.1644 | Training accuracy: 0.1873
14:24:00,140 --- Batch 81/600 | Training loss: 2.6749 | Training accuracy: 0.1618
14:25:22,856 --- Batch 91/600 | Training loss: 2.3424 | Training accuracy: 0.2151
14:25:23,582 --- Validation loss: 2.7266 | Validation accuracy: 0.1752
14:26:42,381 --- Batch 101/600 | Training loss: 2.3940 | Training accuracy: 0.2325
14:28:00,7 --- Batch 111/600 | Training loss: 2.4522 | Training accuracy: 0.2171
14:29:21,220 --- Batch 121/600 | Training loss: 2.2333 | Training accuracy: 0.2187
14:29:21,921 --- Validation loss: 2.9888 | Validation accuracy: 0.0967
14:30:43,696 --- Batch 131/600 | Training loss: 2.2291 | Training accuracy: 0.2490
14:32:05,211 --- Batch 141/600 | Training loss: 2.3012 | Training accuracy: 0.2516
14:33:23,791 --- Batch 151/600 | Training loss: 2.4090 | Training accuracy: 0.2660
14:33:24,539 --- Validation loss: 2.7168 | Validation accuracy: 0.1192
14:34:36,673 --- Batch 161/600 | Training loss: 2.5514 | Training accuracy: 0.2366
14:35:51,432 --- Batch 171/600 | Training loss: 2.0908 | Training accuracy: 0.2640
14:37:12,931 --- Batch 181/600 | Training loss: 2.5753 | Training accuracy: 0.2821
14:37:13,641 --- Validation loss: 2.3877 | Validation accuracy: 0.1561
14:38:27,832 --- Batch 191/600 | Training loss: 2.3129 | Training accuracy: 0.2570
14:39:46,444 --- Batch 201/600 | Training loss: 2.0406 | Training accuracy: 0.2699
14:41:01,253 --- Batch 211/600 | Training loss: 2.2044 | Training accuracy: 0.2947
14:41:01,937 --- Validation loss: 3.0215 | Validation accuracy: 0.1692
14:42:17,275 --- Batch 221/600 | Training loss: 2.2658 | Training accuracy: 0.2995
14:43:36,29 --- Batch 231/600 | Training loss: 2.1989 | Training accuracy: 0.3059
14:44:55,843 --- Batch 241/600 | Training loss: 2.1246 | Training accuracy: 0.3370
14:44:56,577 --- Validation loss: 2.8436 | Validation accuracy: 0.1329
14:46:14,866 --- Batch 251/600 | Training loss: 2.2317 | Training accuracy: 0.3826
14:47:31,574 --- Batch 261/600 | Training loss: 2.1508 | Training accuracy: 0.3708
14:48:49,652 --- Batch 271/600 | Training loss: 1.8507 | Training accuracy: 0.3523
14:48:50,391 --- Validation loss: 4.1461 | Validation accuracy: 0.0705
14:50:14,580 --- Batch 281/600 | Training loss: 2.0446 | Training accuracy: 0.4278
14:51:36,77 --- Batch 291/600 | Training loss: 1.9370 | Training accuracy: 0.3721
14:52:53,921 --- Batch 301/600 | Training loss: 1.9338 | Training accuracy: 0.3895
14:52:54,640 --- Validation loss: 2.9321 | Validation accuracy: 0.2501
14:54:13,445 --- Batch 311/600 | Training loss: 1.6874 | Training accuracy: 0.4707
14:55:34,274 --- Batch 321/600 | Training loss: 1.7763 | Training accuracy: 0.4525
14:56:47,195 --- Batch 331/600 | Training loss: 2.2388 | Training accuracy: 0.4052
14:56:47,888 --- Validation loss: 2.4191 | Validation accuracy: 0.1797
14:58:03,673 --- Batch 341/600 | Training loss: 2.1829 | Training accuracy: 0.3498
14:59:19,111 --- Batch 351/600 | Training loss: 1.9353 | Training accuracy: 0.4119
15:00:42,506 --- Batch 361/600 | Training loss: 1.6677 | Training accuracy: 0.4726
15:00:43,229 --- Validation loss: 2.0665 | Validation accuracy: 0.3827
15:02:03,118 --- Batch 371/600 | Training loss: 1.8860 | Training accuracy: 0.4497
15:03:22,547 --- Batch 381/600 | Training loss: 1.7688 | Training accuracy: 0.4599
15:04:44,835 --- Batch 391/600 | Training loss: 1.5931 | Training accuracy: 0.5498
15:04:45,759 --- Validation loss: 2.7105 | Validation accuracy: 0.1674
15:06:09,177 --- Batch 401/600 | Training loss: 1.7833 | Training accuracy: 0.4933
15:07:28,224 --- Batch 411/600 | Training loss: 1.6471 | Training accuracy: 0.5112
15:08:41,436 --- Batch 421/600 | Training loss: 1.7223 | Training accuracy: 0.4982
15:08:42,173 --- Validation loss: 2.4663 | Validation accuracy: 0.2180
15:10:06,22 --- Batch 431/600 | Training loss: 1.5912 | Training accuracy: 0.5708
15:11:27,664 --- Batch 441/600 | Training loss: 2.1159 | Training accuracy: 0.3820
15:12:49,369 --- Batch 451/600 | Training loss: 1.7162 | Training accuracy: 0.5216
15:12:50,93 --- Validation loss: 2.5509 | Validation accuracy: 0.2125
15:14:09,77 --- Batch 461/600 | Training loss: 1.3661 | Training accuracy: 0.6070
15:15:24,582 --- Batch 471/600 | Training loss: 1.4162 | Training accuracy: 0.5468
15:16:49,311 --- Batch 481/600 | Training loss: 1.7818 | Training accuracy: 0.5265
15:16:50,89 --- Validation loss: 2.8044 | Validation accuracy: 0.2061
15:17:57,319 --- Batch 491/600 | Training loss: 1.4793 | Training accuracy: 0.6034
15:19:16,148 --- Batch 501/600 | Training loss: 1.8665 | Training accuracy: 0.4510
15:20:38,558 --- Batch 511/600 | Training loss: 1.8250 | Training accuracy: 0.5234
15:20:39,291 --- Validation loss: 2.7068 | Validation accuracy: 0.1740
15:21:58,645 --- Batch 521/600 | Training loss: 1.5148 | Training accuracy: 0.6106
15:23:20,537 --- Batch 531/600 | Training loss: 1.5711 | Training accuracy: 0.5484
15:24:42,874 --- Batch 541/600 | Training loss: 1.5096 | Training accuracy: 0.5841
15:24:43,648 --- Validation loss: 1.8726 | Validation accuracy: 0.4275
15:26:03,199 --- Batch 551/600 | Training loss: 1.5127 | Training accuracy: 0.5748
15:27:29,474 --- Batch 561/600 | Training loss: 1.5198 | Training accuracy: 0.5962
15:28:51,729 --- Batch 571/600 | Training loss: 1.8320 | Training accuracy: 0.4682
15:28:52,632 --- Validation loss: 2.9568 | Validation accuracy: 0.2192
15:30:14,898 --- Batch 581/600 | Training loss: 1.3506 | Training accuracy: 0.6295
15:31:30,34 --- Batch 591/600 | Training loss: 1.4186 | Training accuracy: 0.6338
15:32:42,579 --- Training time: 79.45 mins
15:32:43,166 --- Model saved
15:52:09,915 ---  
15:52:09,915 --- =========================================================================
15:52:09,915 --- New run started with id 124813
15:52:09,917 --- Epochs strategy
15:52:09,919 ---  
15:52:12,512 --- Loading prepared data from dir
15:53:40,913 --- Training dataloaders ready
15:53:40,914 --- Validation dataloaders ready
15:53:40,914 --- Loading prepared testing data from dir
15:55:39,766 --- Testing dataloaders ready
15:55:39,766 ---  
15:55:39,799 --- Using cuda:2
15:55:55,525 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
15:55:55,526 --- Number of training batches: 11
15:55:55,527 --- Number of validation batches: 7
15:55:55,529 --- Number of epochs: 50
15:55:55,530 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
15:55:55,533 ---  
15:55:55,538 --- Epoch: 1
15:58:12,778 --- Epoch time: 137.24 s
15:58:12,779 --- Training loss: 2.9183 | Training accuracy: 0.14324230497533624 | Validation loss: 3.3128 | Validation accuracy: 0.0771
15:58:12,781 ---  
15:58:12,784 --- Epoch: 2
16:00:24,967 --- Epoch time: 132.18 s
16:00:24,968 --- Training loss: 2.6446 | Training accuracy: 0.15863138979131525 | Validation loss: 2.9077 | Validation accuracy: 0.1352
16:00:24,969 ---  
16:00:24,972 --- Epoch: 3
16:02:38,889 --- Epoch time: 133.92 s
16:02:38,892 --- Training loss: 2.6789 | Training accuracy: 0.16960460489446466 | Validation loss: 3.0288 | Validation accuracy: 0.1413
16:02:38,895 ---  
16:02:38,897 --- Epoch: 4
16:04:52,717 --- Epoch time: 133.82 s
16:04:52,717 --- Training loss: 2.5443 | Training accuracy: 0.17897640575062146 | Validation loss: 2.7996 | Validation accuracy: 0.1678
16:04:52,721 ---  
16:04:52,724 --- Epoch: 5
16:07:06,305 --- Epoch time: 133.58 s
16:07:06,306 --- Training loss: 2.6199 | Training accuracy: 0.18012253804640335 | Validation loss: 2.5598 | Validation accuracy: 0.1597
16:07:06,307 ---  
16:07:06,309 --- Epoch: 6
16:09:19,640 --- Epoch time: 133.33 s
16:09:19,641 --- Training loss: 2.5161 | Training accuracy: 0.19154919277537952 | Validation loss: 2.6527 | Validation accuracy: 0.1569
16:09:19,644 ---  
16:09:19,645 --- Epoch: 7
16:11:32,862 --- Epoch time: 133.22 s
16:11:32,862 --- Training loss: 2.5405 | Training accuracy: 0.1876740022139116 | Validation loss: 2.7445 | Validation accuracy: 0.1631
16:11:32,865 ---  
16:11:32,867 --- Epoch: 8
16:13:46,525 --- Epoch time: 133.66 s
16:13:46,526 --- Training loss: 2.5097 | Training accuracy: 0.19396400451660156 | Validation loss: 2.6926 | Validation accuracy: 0.1474
16:13:46,529 ---  
16:13:46,531 --- Epoch: 9
16:16:00,291 --- Epoch time: 133.76 s
16:16:00,292 --- Training loss: 2.5018 | Training accuracy: 0.20146482641046698 | Validation loss: 2.6157 | Validation accuracy: 0.1598
16:16:00,294 ---  
16:16:00,296 --- Epoch: 10
16:18:13,745 --- Epoch time: 133.45 s
16:18:13,746 --- Training loss: 2.4047 | Training accuracy: 0.20265312628312546 | Validation loss: 2.6411 | Validation accuracy: 0.1446
16:18:13,747 ---  
16:18:13,751 --- Epoch: 11
16:20:27,784 --- Epoch time: 134.03 s
16:20:27,788 --- Training loss: 2.5107 | Training accuracy: 0.20801511677828702 | Validation loss: 2.7097 | Validation accuracy: 0.1613
16:20:27,790 ---  
16:20:27,793 --- Epoch: 12
16:22:41,917 --- Epoch time: 134.12 s
16:22:41,924 --- Training loss: 2.5131 | Training accuracy: 0.21097922325134277 | Validation loss: 2.7297 | Validation accuracy: 0.1425
16:22:41,926 ---  
16:22:41,928 --- Epoch: 13
16:24:53,156 --- Epoch time: 131.23 s
16:24:53,157 --- Training loss: 2.4269 | Training accuracy: 0.21929983659224075 | Validation loss: 2.6365 | Validation accuracy: 0.1524
16:24:53,159 ---  
16:24:53,161 --- Epoch: 14
16:27:06,341 --- Epoch time: 133.18 s
16:27:06,342 --- Training loss: 2.4120 | Training accuracy: 0.22512752359563654 | Validation loss: 2.6461 | Validation accuracy: 0.1481
16:27:06,344 ---  
16:27:06,344 --- Epoch: 15
16:29:20,375 --- Epoch time: 134.03 s
16:29:20,375 --- Training loss: 2.3979 | Training accuracy: 0.23375621708956631 | Validation loss: 2.7141 | Validation accuracy: 0.1648
16:29:20,377 ---  
16:29:20,379 --- Epoch: 16
16:31:34,659 --- Epoch time: 134.28 s
16:31:34,660 --- Training loss: 2.3435 | Training accuracy: 0.24469171870838513 | Validation loss: 2.8117 | Validation accuracy: 0.1496
16:31:34,662 ---  
16:31:34,665 --- Epoch: 17
16:33:51,44 --- Epoch time: 136.38 s
16:33:51,45 --- Training loss: 2.3378 | Training accuracy: 0.2510773918845437 | Validation loss: 2.7004 | Validation accuracy: 0.1507
16:33:51,48 ---  
16:33:51,50 --- Epoch: 18
16:36:06,186 --- Epoch time: 135.14 s
16:36:06,187 --- Training loss: 2.3512 | Training accuracy: 0.26227363673123444 | Validation loss: 2.6643 | Validation accuracy: 0.1556
16:36:06,190 ---  
16:36:06,192 --- Epoch: 19
16:38:20,668 --- Epoch time: 134.48 s
16:38:20,668 --- Training loss: 2.2473 | Training accuracy: 0.27884411811828613 | Validation loss: 2.7651 | Validation accuracy: 0.1457
16:38:20,670 ---  
16:38:20,672 --- Epoch: 20
16:40:34,698 --- Epoch time: 134.02 s
16:40:34,713 --- Training loss: 2.2537 | Training accuracy: 0.2934893478046764 | Validation loss: 2.6841 | Validation accuracy: 0.1910
16:40:34,716 ---  
16:40:34,719 --- Epoch: 21
16:42:50,726 --- Epoch time: 136.01 s
16:42:50,727 --- Training loss: 2.3226 | Training accuracy: 0.3002536513588645 | Validation loss: 2.7259 | Validation accuracy: 0.1823
16:42:50,729 ---  
16:42:50,733 --- Epoch: 22
16:45:03,829 --- Epoch time: 133.10 s
16:45:03,830 --- Training loss: 2.2130 | Training accuracy: 0.3244097016074441 | Validation loss: 2.7477 | Validation accuracy: 0.1520
16:45:03,832 ---  
16:45:03,836 --- Epoch: 23
16:47:19,730 --- Epoch time: 135.89 s
16:47:19,731 --- Training loss: 2.1077 | Training accuracy: 0.341872128573331 | Validation loss: 2.8098 | Validation accuracy: 0.1732
16:47:19,734 ---  
16:47:19,737 --- Epoch: 24
16:49:36,308 --- Epoch time: 136.57 s
16:49:36,310 --- Training loss: 2.0999 | Training accuracy: 0.35779077356511896 | Validation loss: 2.8288 | Validation accuracy: 0.1447
16:49:36,314 ---  
16:49:36,317 --- Epoch: 25
16:51:53,14 --- Epoch time: 136.70 s
16:51:53,27 --- Training loss: 2.0000 | Training accuracy: 0.3747169754721902 | Validation loss: 2.8505 | Validation accuracy: 0.2046
16:51:53,28 ---  
16:51:53,30 --- Epoch: 26
16:54:08,839 --- Epoch time: 135.81 s
16:54:08,840 --- Training loss: 2.1159 | Training accuracy: 0.39421783794056287 | Validation loss: 2.8675 | Validation accuracy: 0.1989
16:54:08,847 ---  
16:54:08,852 --- Epoch: 27
16:56:24,322 --- Epoch time: 135.47 s
16:56:24,323 --- Training loss: 2.0177 | Training accuracy: 0.3961241028525613 | Validation loss: 2.7110 | Validation accuracy: 0.2063
16:56:24,324 ---  
16:56:24,326 --- Epoch: 28
16:58:39,914 --- Epoch time: 135.59 s
16:58:39,915 --- Training loss: 1.9594 | Training accuracy: 0.41625998236916284 | Validation loss: 2.5501 | Validation accuracy: 0.2077
16:58:39,917 ---  
16:58:39,919 --- Epoch: 29
17:00:55,969 --- Epoch time: 136.05 s
17:00:55,970 --- Training loss: 1.9741 | Training accuracy: 0.42006046121770685 | Validation loss: 2.8703 | Validation accuracy: 0.2047
17:00:55,971 ---  
17:00:55,974 --- Epoch: 30
17:03:08,835 --- Epoch time: 132.86 s
17:03:08,836 --- Training loss: 1.9295 | Training accuracy: 0.43192989175969904 | Validation loss: 2.8253 | Validation accuracy: 0.2022
17:03:08,838 ---  
17:03:08,840 --- Epoch: 31
17:05:20,122 --- Epoch time: 131.28 s
17:05:20,123 --- Training loss: 1.8529 | Training accuracy: 0.43873314423994586 | Validation loss: 2.7762 | Validation accuracy: 0.2067
17:05:20,125 ---  
17:05:20,128 --- Epoch: 32
17:07:33,517 --- Epoch time: 133.39 s
17:07:33,518 --- Training loss: 1.9144 | Training accuracy: 0.4506934772838246 | Validation loss: 2.8321 | Validation accuracy: 0.2043
17:07:33,521 ---  
17:07:33,524 --- Epoch: 33
17:09:46,873 --- Epoch time: 133.35 s
17:09:46,874 --- Training loss: 1.7645 | Training accuracy: 0.46431420066139917 | Validation loss: 2.8205 | Validation accuracy: 0.2035
17:09:46,878 ---  
17:09:46,880 --- Epoch: 34
17:11:59,264 --- Epoch time: 132.38 s
17:11:59,267 --- Training loss: 1.8284 | Training accuracy: 0.475316351110285 | Validation loss: 2.7602 | Validation accuracy: 0.2050
17:11:59,268 ---  
17:11:59,270 --- Epoch: 35
17:14:12,628 --- Epoch time: 133.36 s
17:14:12,629 --- Training loss: 1.7000 | Training accuracy: 0.4820610393177379 | Validation loss: 2.7310 | Validation accuracy: 0.2271
17:14:12,631 ---  
17:14:12,633 --- Epoch: 36
17:16:28,325 --- Epoch time: 135.69 s
17:16:28,326 --- Training loss: 1.7489 | Training accuracy: 0.4863048033280806 | Validation loss: 2.7492 | Validation accuracy: 0.2352
17:16:28,328 ---  
17:16:28,330 --- Epoch: 37
17:18:44,362 --- Epoch time: 136.03 s
17:18:44,362 --- Training loss: 1.7297 | Training accuracy: 0.49477538195523346 | Validation loss: 2.7886 | Validation accuracy: 0.2158
17:18:44,364 ---  
17:18:44,366 --- Epoch: 38
17:21:00,161 --- Epoch time: 135.79 s
17:21:00,162 --- Training loss: 1.6819 | Training accuracy: 0.5129548853093927 | Validation loss: 2.7880 | Validation accuracy: 0.2321
17:21:00,166 ---  
17:21:00,168 --- Epoch: 39
17:23:15,965 --- Epoch time: 135.80 s
17:23:15,967 --- Training loss: 1.6908 | Training accuracy: 0.5108499960465864 | Validation loss: 2.7488 | Validation accuracy: 0.1982
17:23:15,972 ---  
17:23:15,975 --- Epoch: 40
17:25:32,474 --- Epoch time: 136.50 s
17:25:32,475 --- Training loss: 1.7451 | Training accuracy: 0.5162059610540216 | Validation loss: 2.8021 | Validation accuracy: 0.2373
17:25:32,482 ---  
17:25:32,483 --- Epoch: 41
17:27:48,279 --- Epoch time: 135.80 s
17:27:48,280 --- Training loss: 1.6561 | Training accuracy: 0.5244869318875399 | Validation loss: 2.6378 | Validation accuracy: 0.2404
17:27:48,284 ---  
17:27:48,289 --- Epoch: 42
17:30:03,403 --- Epoch time: 135.11 s
17:30:03,404 --- Training loss: 1.6414 | Training accuracy: 0.5246176286177202 | Validation loss: 2.6618 | Validation accuracy: 0.2265
17:30:03,405 ---  
17:30:03,408 --- Epoch: 43
17:32:18,772 --- Epoch time: 135.36 s
17:32:18,773 --- Training loss: 1.6406 | Training accuracy: 0.5321509187871759 | Validation loss: 2.7084 | Validation accuracy: 0.2292
17:32:18,776 ---  
17:32:18,781 --- Epoch: 44
17:34:35,829 --- Epoch time: 137.05 s
17:34:35,830 --- Training loss: 1.5733 | Training accuracy: 0.5373802185058594 | Validation loss: 2.6338 | Validation accuracy: 0.2464
17:34:35,837 ---  
17:34:35,837 --- Epoch: 45
17:36:51,612 --- Epoch time: 135.77 s
17:36:51,613 --- Training loss: 1.6003 | Training accuracy: 0.548040433363481 | Validation loss: 3.0077 | Validation accuracy: 0.2239
17:36:51,614 ---  
17:36:51,617 --- Epoch: 46
17:39:07,906 --- Epoch time: 136.29 s
17:39:07,907 --- Training loss: 1.5874 | Training accuracy: 0.5561764457009055 | Validation loss: 2.7264 | Validation accuracy: 0.2329
17:39:07,908 ---  
17:39:07,910 --- Epoch: 47
17:41:23,623 --- Epoch time: 135.71 s
17:41:23,624 --- Training loss: 1.5642 | Training accuracy: 0.5612149672074751 | Validation loss: 2.7862 | Validation accuracy: 0.2341
17:41:23,627 ---  
17:41:23,629 --- Epoch: 48
17:43:39,83 --- Epoch time: 135.45 s
17:43:39,84 --- Training loss: 1.5619 | Training accuracy: 0.5642686757174405 | Validation loss: 2.9299 | Validation accuracy: 0.2218
17:43:39,85 ---  
17:43:39,90 --- Epoch: 49
17:45:52,867 --- Epoch time: 133.78 s
17:45:52,867 --- Training loss: 1.5359 | Training accuracy: 0.5668861649253152 | Validation loss: 2.7228 | Validation accuracy: 0.2345
17:45:52,869 ---  
17:45:52,871 --- Epoch: 50
17:48:06,808 --- Epoch time: 133.94 s
17:48:06,809 --- Training loss: 1.5067 | Training accuracy: 0.5717607844959606 | Validation loss: 2.6965 | Validation accuracy: 0.2213
17:48:06,813 ---  
17:48:06,815 --- Training time: 112.19 mins
17:48:07,160 --- Model saved
17:48:07,188 --- Metrics saved
17:49:52,384 ---  
17:49:52,384 --- =========================================================================
17:49:52,384 --- New run started with id 164328
17:49:52,384 --- ID: 164328
17:49:52,384 --- Infinite iterators strategy
17:52:00,266 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
17:52:00,266 --- Training dataloaders ready
17:52:00,266 ---  
17:52:00,266 --- Loading prepared data from dir
17:52:09,49 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
17:52:09,49 --- Validation dataloaders ready
17:52:09,49 ---  
17:52:09,49 --- Loading prepared data from dir
17:52:21,62 --- Testing dataloaders ready
17:52:21,78 ---  
17:52:21,189 --- Using cuda:0
17:52:23,347 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
17:52:23,347 --- Total training batches: 600
17:52:23,347 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
17:52:23,347 ---  
17:52:35,197 --- Batch 1/600 | Training loss: 3.4778 | Training accuracy: 0.0235
17:52:42,61 --- Validation loss: 3.5984 | Validation accuracy: 0.0161
