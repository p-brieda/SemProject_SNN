14:14:17,58 ---  
14:14:17,58 --- =========================================================================
14:14:17,58 --- New run started with id 394634
14:14:17,58 ---  
14:14:29,650 --- Loading prepared data from dir
14:14:29,650 --- Training dataloaders ready
14:14:29,650 --- Validation dataloaders ready
14:14:42,335 --- Testing dataloaders ready
14:14:42,335 ---  
14:14:43,283 --- Using cuda:0
14:14:45,17 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
14:14:45,17 --- Number of training batches: 30
14:14:45,17 --- Number of validation batches: 14
14:14:45,17 --- Number of epochs: 10
14:14:45,17 --- Scheduler: StepLR(step_size=10, gamma=0.1)
14:14:45,17 ---  
14:14:45,17 --- Epoch: 1
14:21:09,756 --- Epoch time: 384.74 s
14:21:09,760 --- Train loss: 2.9132 | Validation loss: 2.6502 | Validation accuracy: 0.1628
14:21:09,760 ---  
14:21:09,760 --- Epoch: 2
14:27:42,615 --- Epoch time: 392.85 s
14:27:42,615 --- Train loss: 2.6877 | Validation loss: 2.7718 | Validation accuracy: 0.1641
14:27:42,615 ---  
14:27:42,615 --- Epoch: 3
14:34:12,290 --- Epoch time: 389.67 s
14:34:12,290 --- Train loss: 2.6841 | Validation loss: 2.7798 | Validation accuracy: 0.1640
14:34:12,290 ---  
14:34:12,290 --- Epoch: 4
14:40:36,337 --- Epoch time: 384.05 s
14:40:36,337 --- Train loss: 2.6764 | Validation loss: 2.8196 | Validation accuracy: 0.1696
14:40:36,337 ---  
14:40:36,337 --- Epoch: 5
15:13:13,759 ---  
15:13:13,764 --- =========================================================================
15:13:13,764 --- New run started with id 106885
15:13:13,764 ---  
15:13:37,992 --- Loading prepared data from dir
15:13:38,75 --- Training dataloaders ready
15:13:38,75 --- Validation dataloaders ready
15:14:09,688 --- Testing dataloaders ready
15:14:09,688 ---  
15:14:09,843 --- Using cuda:0
15:14:13,316 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
15:14:13,316 --- Number of training batches: 30
15:14:13,316 --- Number of validation batches: 14
15:14:13,316 --- Number of epochs: 10
15:14:13,316 --- Scheduler: StepLR(step_size=10, gamma=0.1)
15:14:13,316 ---  
15:14:13,316 --- Epoch: 1
15:20:22,851 --- Epoch time: 369.53 s
15:20:22,860 --- Train loss: 2.8744 | Validation loss: 2.7271 | Validation accuracy: 0.1649
15:20:22,860 ---  
15:20:22,860 --- Epoch: 2
15:26:02,216 --- Epoch time: 339.35 s
15:26:02,218 --- Train loss: 2.6846 | Validation loss: 2.7205 | Validation accuracy: 0.1640
15:26:02,218 ---  
15:26:02,219 --- Epoch: 3
15:31:36,940 --- Epoch time: 334.72 s
15:31:36,944 --- Train loss: 2.8073 | Validation loss: 2.8192 | Validation accuracy: 0.1653
15:31:36,944 ---  
15:31:36,945 --- Epoch: 4
15:37:05,92 --- Epoch time: 328.15 s
15:37:05,93 --- Train loss: 2.7013 | Validation loss: 2.7331 | Validation accuracy: 0.1633
15:37:05,93 ---  
15:37:05,93 --- Epoch: 5
15:42:32,249 --- Epoch time: 327.15 s
15:42:32,250 --- Train loss: 2.6762 | Validation loss: 2.7166 | Validation accuracy: 0.1553
15:42:32,251 ---  
15:42:32,251 --- Epoch: 6
15:48:06,545 --- Epoch time: 334.29 s
15:48:06,558 --- Train loss: 2.7253 | Validation loss: 2.7384 | Validation accuracy: 0.1655
15:48:06,559 ---  
15:48:06,559 --- Epoch: 7
15:53:44,381 --- Epoch time: 337.82 s
15:53:44,383 --- Train loss: 2.6478 | Validation loss: 2.7993 | Validation accuracy: 0.1644
15:53:44,383 ---  
15:53:44,384 --- Epoch: 8
15:59:22,479 --- Epoch time: 338.10 s
15:59:22,482 --- Train loss: 2.6635 | Validation loss: 2.8200 | Validation accuracy: 0.1207
15:59:22,482 ---  
15:59:22,482 --- Epoch: 9
16:04:58,572 --- Epoch time: 336.09 s
16:04:58,574 --- Train loss: 2.7490 | Validation loss: 2.6876 | Validation accuracy: 0.1653
16:04:58,575 ---  
16:04:58,576 --- Epoch: 10
16:10:30,245 --- Epoch time: 331.67 s
16:10:30,249 --- Train loss: 2.5507 | Validation loss: 2.6058 | Validation accuracy: 0.1691
16:10:30,249 ---  
16:10:30,249 --- Training time: 56.28 mins
16:10:30,297 --- Model saved
09:48:57,226 ---  
16:23:10,751 ---  
16:23:10,751 --- =========================================================================
16:23:10,751 --- New run started with id 451123
16:23:10,751 ---  
16:27:01,613 ---  
16:27:01,613 --- =========================================================================
16:27:01,613 --- New run started with id 915640
16:27:01,613 ---  
16:27:12,968 --- Loading prepared data from dir
16:27:12,968 --- Training dataloaders ready
16:27:12,968 --- Validation dataloaders ready
16:27:22,928 --- Testing dataloaders ready
16:27:22,928 ---  
16:27:23,0 --- Using cuda:0
16:27:24,356 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:27:24,356 --- Number of training batches: 30
16:27:24,356 --- Number of validation batches: 14
16:27:24,356 --- Number of epochs: 10
16:27:24,356 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:27:24,356 ---  
16:27:24,356 --- Epoch: 1
16:32:17,482 ---  
16:32:17,482 --- =========================================================================
16:32:17,482 --- New run started with id 434948
16:32:17,482 ---  
16:32:28,469 --- Loading prepared data from dir
16:32:28,470 --- Training dataloaders ready
16:32:28,471 --- Validation dataloaders ready
16:32:37,397 --- Testing dataloaders ready
16:32:37,397 ---  
16:32:37,445 --- Using cuda:0
16:32:38,362 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:32:38,362 --- Number of training batches: 30
16:32:38,362 --- Number of validation batches: 14
16:32:38,362 --- Number of epochs: 10
16:32:38,362 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:32:38,362 ---  
16:32:38,362 --- Epoch: 1
16:41:59,731 ---  
16:41:59,731 --- =========================================================================
16:41:59,731 --- New run started with id 452988
16:41:59,731 ---  
16:42:09,147 --- Loading prepared data from dir
16:42:09,147 --- Training dataloaders ready
16:42:09,147 --- Validation dataloaders ready
16:42:18,596 --- Testing dataloaders ready
16:42:18,596 ---  
16:42:18,641 --- Using cuda:0
16:42:19,577 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:42:19,577 --- Number of training batches: 30
16:42:19,577 --- Number of validation batches: 14
16:42:19,577 --- Number of epochs: 10
16:42:19,577 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:42:19,577 ---  
16:42:19,577 --- Epoch: 1
16:44:04,278 ---  
16:44:04,278 --- =========================================================================
16:44:04,278 --- New run started with id 538834
16:44:04,278 ---  
16:44:13,707 --- Loading prepared data from dir
16:44:13,722 --- Training dataloaders ready
16:44:13,722 --- Validation dataloaders ready
16:44:22,723 --- Testing dataloaders ready
16:44:22,723 ---  
16:44:22,769 --- Using cuda:0
16:44:23,680 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:44:23,680 --- Number of training batches: 30
16:44:23,680 --- Number of validation batches: 14
16:44:23,680 --- Number of epochs: 10
16:44:23,680 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:44:23,680 ---  
16:44:23,680 --- Epoch: 1
16:45:03,155 ---  
16:45:03,155 --- =========================================================================
16:45:03,155 --- New run started with id 664905
16:45:03,155 ---  
16:45:12,355 --- Loading prepared data from dir
16:45:12,355 --- Training dataloaders ready
16:45:12,355 --- Validation dataloaders ready
16:45:21,282 --- Testing dataloaders ready
16:45:21,282 ---  
16:45:21,315 --- Using cuda:0
16:45:22,255 --- Optimizer: AdamW(lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
16:45:22,255 --- Number of training batches: 30
16:45:22,255 --- Number of validation batches: 14
16:45:22,255 --- Number of epochs: 10
16:45:22,255 --- Scheduler: StepLR(step_size=10, gamma=0.1)
16:45:22,255 ---  
16:45:22,255 --- Epoch: 1
16:47:51,692 --- Epoch time: 149.44 s
16:47:51,702 --- Train loss: 2.8342 | Validation loss: 2.8011 | Validation accuracy: 0.1502
16:47:51,702 ---  
16:47:51,702 --- Epoch: 2
16:50:26,657 --- Epoch time: 154.95 s
16:50:26,657 --- Train loss: 2.7936 | Validation loss: 2.6426 | Validation accuracy: 0.1682
16:50:26,657 ---  
16:50:26,657 --- Epoch: 3
16:52:55,468 --- Epoch time: 148.81 s
16:52:55,469 --- Train loss: 2.6431 | Validation loss: 2.8104 | Validation accuracy: 0.1678
16:52:55,469 ---  
16:52:55,469 --- Epoch: 4
16:55:20,883 --- Epoch time: 145.41 s
16:55:20,883 --- Train loss: 2.6178 | Validation loss: 2.7072 | Validation accuracy: 0.1640
16:55:20,884 ---  
16:55:20,884 --- Epoch: 5
16:57:47,952 --- Epoch time: 147.07 s
16:57:47,954 --- Train loss: 2.6719 | Validation loss: 2.7374 | Validation accuracy: 0.1568
16:57:47,955 ---  
16:57:47,955 --- Epoch: 6
17:00:15,5 --- Epoch time: 147.05 s
17:00:15,6 --- Train loss: 2.6401 | Validation loss: 2.8167 | Validation accuracy: 0.1636
17:00:15,6 ---  
17:00:15,7 --- Epoch: 7
17:02:41,359 --- Epoch time: 146.35 s
17:02:41,361 --- Train loss: 2.7423 | Validation loss: 2.6569 | Validation accuracy: 0.1498
17:02:41,361 ---  
17:02:41,361 --- Epoch: 8
10:53:05,739 ---  
10:53:05,739 --- =========================================================================
10:53:05,739 --- New run started with id 525137
10:53:05,739 ---  
10:53:07,653 --- Loading prepared data from dir
10:53:17,771 --- Training dataloaders ready
10:53:17,771 --- Validation dataloaders ready
10:53:17,771 --- Loading prepared testing data from dir
10:53:27,263 --- Testing dataloaders ready
10:53:27,263 ---  
10:53:27,324 --- Using cuda:0
10:53:28,623 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
10:53:28,623 --- Number of training batches: 11
10:53:28,623 --- Number of validation batches: 7
10:53:28,623 --- Number of epochs: 100
10:53:28,623 --- Scheduler: StepLR(step_size=10, gamma=0.1)
10:53:28,623 ---  
10:53:28,623 --- Epoch: 1
10:56:54,284 --- Epoch time: 205.66 s
10:56:54,284 --- Training loss: 2.9486 | Training accuracy: 0.10586257414384322 | Validation loss: 3.3072 | Validation accuracy: 0.0611
10:56:54,284 ---  
10:56:54,284 --- Epoch: 2
11:00:19,934 --- Epoch time: 205.65 s
11:00:19,934 --- Training loss: 2.7202 | Training accuracy: 0.16262356801466507 | Validation loss: 2.9285 | Validation accuracy: 0.0797
11:00:19,934 ---  
11:00:19,941 --- Epoch: 3
11:03:40,558 --- Epoch time: 200.62 s
11:03:40,559 --- Training loss: 2.7241 | Training accuracy: 0.1406726078553633 | Validation loss: 2.8814 | Validation accuracy: 0.1200
11:03:40,559 ---  
11:03:40,560 --- Epoch: 4
12:14:43,348 ---  
12:14:43,348 --- =========================================================================
12:14:43,349 --- New run started with id 908027
12:14:43,350 ---  
12:14:45,267 --- Loading prepared data from dir
12:14:54,34 --- Loading prepared testing data from dir
12:15:04,753 --- Training dataloaders ready
12:15:04,753 ---  
12:15:04,753 --- Loading prepared testing data from dir
12:15:20,579 --- Validation dataloaders ready
12:15:20,579 ---  
12:15:20,579 --- Loading prepared testing data from dir
12:15:36,55 --- Testing dataloaders ready
12:15:36,55 ---  
12:15:36,181 --- Using cuda:0
12:15:38,70 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:15:38,70 --- Total training batches: 600
12:15:38,70 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:15:38,70 ---  
12:16:09,250 ---  
12:16:09,250 --- =========================================================================
12:16:09,250 --- New run started with id 222127
12:16:09,250 ---  
12:16:10,769 --- Loading prepared data from dir
12:16:18,159 --- Loading prepared testing data from dir
12:16:27,113 --- Training dataloaders ready
12:16:27,113 ---  
12:16:27,113 --- Loading prepared testing data from dir
12:16:41,32 --- Validation dataloaders ready
12:16:41,33 ---  
12:16:41,33 --- Loading prepared testing data from dir
12:16:56,104 --- Testing dataloaders ready
12:16:56,104 ---  
12:16:56,189 --- Using cuda:0
12:16:57,649 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:16:57,649 --- Total training batches: 600
12:16:57,649 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:16:57,649 ---  
12:20:08,411 ---  
12:20:08,411 --- =========================================================================
12:20:08,411 --- New run started with id 438532
12:20:08,411 ---  
12:20:12,262 --- Loading prepared data from dir
12:20:19,578 --- Loading prepared testing data from dir
12:20:28,902 --- Training dataloaders ready
12:20:28,902 ---  
12:20:28,902 --- Loading prepared testing data from dir
12:20:40,647 --- Validation dataloaders ready
12:20:40,647 ---  
12:20:40,662 --- Loading prepared testing data from dir
12:20:53,696 --- Testing dataloaders ready
12:20:53,699 ---  
12:20:53,831 --- Using cuda:0
12:20:55,784 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:20:55,784 --- Total training batches: 600
12:20:55,784 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:20:55,784 ---  
12:30:50,263 ---  
12:30:50,264 --- =========================================================================
12:30:50,264 --- New run started with id 590950
12:30:50,264 ---  
12:33:00,571 --- Training dataloaders ready
12:33:00,571 ---  
12:35:18,805 --- Validation dataloaders ready
12:35:18,805 ---  
12:37:41,819 --- Testing dataloaders ready
12:37:41,819 ---  
12:37:41,920 --- Using cuda:0
12:37:43,282 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:37:43,282 --- Total training batches: 600
12:37:43,282 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:37:43,282 ---  
12:39:08,478 ---  
12:39:08,478 --- =========================================================================
12:39:08,478 --- New run started with id 439961
12:39:08,478 ---  
12:39:11,756 --- Loading prepared training data from dir
12:39:19,284 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:39:19,306 --- Training dataloaders ready
12:39:19,306 ---  
12:39:19,306 --- Loading prepared validation data from dir
12:39:29,83 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:39:29,83 --- Validation dataloaders ready
12:39:29,83 ---  
12:39:29,83 --- Loading prepared testing data from dir
12:39:40,969 --- Testing dataloaders ready
12:39:40,969 ---  
12:39:41,34 --- Using cuda:0
12:39:43,75 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:39:43,75 --- Total training batches: 600
12:39:43,75 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:39:43,75 ---  
12:40:01,119 --- Batch 1/600 | Training loss: 3.421 | Training accuracy: 0.032
12:46:39,622 ---  
12:46:39,622 --- =========================================================================
12:46:39,622 --- New run started with id 335772
12:46:39,622 ---  
12:46:41,885 --- Loading prepared training data from dir
12:46:50,755 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:46:50,755 --- Training dataloaders ready
12:46:50,755 ---  
12:46:50,755 --- Loading prepared validation data from dir
12:47:00,593 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:47:00,593 --- Validation dataloaders ready
12:47:00,593 ---  
12:47:00,593 --- Loading prepared testing data from dir
12:47:12,15 --- Testing dataloaders ready
12:47:12,16 ---  
12:47:12,120 --- Using cuda:0
12:47:14,4 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:47:14,4 --- Total training batches: 600
12:47:14,4 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:47:14,5 ---  
12:47:33,9 --- Batch 1/600 | Training loss: 3.452 | Training accuracy: 0.097
12:47:33,749 --- Validation loss: 3.418 | Validation accuracy: 0.018
12:49:37,617 ---  
12:49:37,617 --- =========================================================================
12:49:37,617 --- New run started with id 218091
12:49:37,617 --- ID: 218091
12:49:37,617 --- Infinite iterators strategy
12:49:39,647 --- Loading prepared training data from dir
12:49:46,887 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:49:46,887 --- Training dataloaders ready
12:49:46,887 ---  
12:49:46,892 --- Loading prepared validation data from dir
12:49:55,763 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:49:55,763 --- Validation dataloaders ready
12:49:55,763 ---  
12:49:55,763 --- Loading prepared testing data from dir
12:50:06,559 --- Testing dataloaders ready
12:50:06,559 ---  
12:50:06,670 --- Using cuda:0
12:50:08,232 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:50:08,232 --- Total training batches: 600
12:50:08,232 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:50:08,232 ---  
12:50:27,101 --- Batch 1/600 | Training loss: 3.436 | Training accuracy: 0.053
12:50:27,813 --- Validation loss: 3.894 | Validation accuracy: 0.000
12:53:47,28 ---  
12:53:47,28 --- =========================================================================
12:53:47,28 --- New run started with id 810262
12:53:47,28 --- ID: 810262
12:53:47,28 --- Infinite iterators strategy
12:53:48,857 --- Loading prepared training data from dir
12:53:55,842 --- Viable training days: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
12:53:55,842 --- Training dataloaders ready
12:53:55,842 ---  
12:53:55,842 --- Loading prepared validation data from dir
12:54:04,685 --- Viable validation days: [1, 2, 3, 4, 5, 6, 7]
12:54:04,685 --- Validation dataloaders ready
12:54:04,685 ---  
12:54:04,685 --- Loading prepared testing data from dir
12:54:15,504 --- Testing dataloaders ready
12:54:15,504 ---  
12:54:15,565 --- Using cuda:0
12:54:16,826 --- Optimizer: AdamW(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
12:54:16,826 --- Total training batches: 600
12:54:16,826 --- Scheduler: LambdaLR(lr_lambda=lambda i: (1 - i/100000))
12:54:16,826 ---  
12:54:35,646 --- Batch 1/600 | Training loss: 3.848 | Training accuracy: 0.035
12:54:36,441 --- Validation loss: 3.481 | Validation accuracy: 0.049
12:57:28,320 --- Batch 11/600 | Training loss: 2.837 | Training accuracy: 0.182
13:00:13,700 --- Batch 21/600 | Training loss: 2.400 | Training accuracy: 0.165
13:02:47,814 --- Batch 31/600 | Training loss: 2.912 | Training accuracy: 0.180
